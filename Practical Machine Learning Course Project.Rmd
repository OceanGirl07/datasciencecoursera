---
title: "Practical Machine Learning Course Project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. 

We use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways (one set of ten repetitions for correct lifts and incorrect lifts respectively). More information is available from this website: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).  The data comes from this paper: 

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.

More information is available at <http://groupware.les.inf.puc-rio.br/har#weight_lifting_exercises#ixzz4syg2fxQj>

We use this data to predict the manner in which participants did the exercise (this is the "classe" variable in the training set), basing the prediction on any of the other variables.


## Getting and Cleaning Data

We load the required R packages and download the training and testing data sets from the provided URLs: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv and https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv.

```{r cars}
library(caret); library(rpart); library(rpart.plot); library(randomForest); library(e1071); library(ggplot2);
# treat empty values as NA
trainingInit <- read.csv('./pml-training.csv', na.strings=c("NA",""), header = TRUE)
testing <- read.csv('./pml-testing.csv', na.strings=c("NA",""), header = TRUE)

```
The training data set includes 19622 observations of 160 variables.  The testing set includes 20 observations of the same variables (note, these are used in the accompanying quiz).  Both sets contain "NA" so must be cleaned.  We also notice that the first seven variables (such as 'user_name' and '*timestamp') should not be used for prediction.  We delete predictors with missing values and eliminate the first seven variables in both sets.  

```{r}
# Delete any columns that contain NAs
trainingInit <- trainingInit[, colSums(is.na(trainingInit)) == 0]
testing <- testing[, colSums(is.na(testing)) == 0]
# Remove first seven features
trainingInit <- trainingInit[, -c(1:7)]
testing <- testing[, -c(1:7)]
```

This reduces our data to 53 varaibles.  The training and testing sets agree on all but the last variable (classe vs problem_id).

We will split the training data into training (70%) and validation (30%) subsets for crossvalidation, and save the provided testing set.  We will do an out-of-sample test with the testing subset.

```{r}
set.seed(123)
inTrain <- createDataPartition(y = trainingInit$classe, p = .7, list = FALSE)
training <- trainingInit[inTrain, ]
validation <- trainingInit[-inTrain, ]
```


## Model Building
We will use two algorithms: classification trees (method = rpart) and random forests (method = rf).  We will select the best model based on the out of sample testing. 

```{r}
#control computational nuinces of train function so rpart does not take too long
control <- trainControl(method = "cv", number = 5)
modfit <- train(classe ~., method = "rpart", data = training, trControl = control)
print(modfit$finalModel, digits = 4)

# predict outcomes using validation set
predict1 <- predict(modfit, validation)

# confusion matrix
cm <- confusionMatrix(validation$classe, predict1)

# get accuracy
accuracy_rpart <- cm$overall[1]

print(cm)
print(accuracy_rpart)
```
The accuracy rate is around .55, so the out of sample error rate if .45.  The classification tree is not a good predictor.  We would do about as well with a random coin flip.



For random forests, change the method in the code.
```{r}
modfit2 <- train(classe ~., method = "rf", data = training, trControl = control)
print(modfit$finalModel, digits = 4)

# predict outcomes using validation set
predict2 <- predict(modfit2, validation)

# confusion matrix
cm2 <- confusionMatrix(validation$classe, predict2)

# get accuracy
accuracy_rf <- cm2$overall[1]

print(cm2)
print(accuracy_rf)
```

For this data set, the random forest method is much more accurate than the classification tree method.  The accuracy rate is .993, so the out of sample error rate is .007.  This could be because many predictors are highly correlcated.  Random forests choose a subset of predictors at each split and decorrelate the tree and correct for decision tree's habits of overfitting the training set (low bias, high variance).  Random forests are a way of averaging multiple deep decision trees with the goal of reducing variance.  This comes with the cost of a small increase in the bias and some loss of interpretability, but the performance of the model is usually better.  

## Prediction on Testing Set

We use random forests to predict the outcome of the class variable for the testing set.  (Solutions to the quiz.)

```{r}
predict(modfit2, testing)
```
